{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 959122,
     "status": "ok",
     "timestamp": 1737776403461,
     "user": {
      "displayName": "Ruoqi Wei",
      "userId": "06248467831337262349"
     },
     "user_tz": 300
    },
    "id": "5zOzDFZJZfaU",
    "outputId": "d2c976e3-fccd-4591-9245-0467ebb7751e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier__max_depth': None, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 100}\n",
      "Best Score:  0.8946488906001828\n",
      "Accuracy: Mean=0.8450120192307692, 95% CI=(0.8318960336538462, 0.8544170673076923)\n",
      "Precision: Mean=0.9384197883573051, 95% CI=(0.9252238647013717, 0.9479235258685723)\n",
      "Recall: Mean=0.7137192533087208, 95% CI=(0.6859680013309672, 0.7342510485780072)\n",
      "F1: Mean=0.8106716991823179, 95% CI=(0.7918895830625711, 0.8243219158649506)\n",
      "Specificity: Mean=0.9591649711402059, 95% CI=(0.9498505455553304, 0.967382274001964)\n",
      "Roc_auc: Mean=0.9467531422120677, 95% CI=(0.9374200838358637, 0.9545132094463199)\n",
      "Auprc: Mean=0.907748938775204, 95% CI=(0.8919393323985926, 0.9261643761661414)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for data processing, machine learning, and evaluation\n",
    "import pandas as pd  # For handling tabular data\n",
    "import numpy as np  # For numerical operations\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # For converting text into numerical features\n",
    "from sklearn.ensemble import RandomForestClassifier  # For using the Random Forest algorithm\n",
    "from sklearn.pipeline import Pipeline  # For combining preprocessing and modeling into a single workflow\n",
    "from sklearn.model_selection import GridSearchCV  # For finding the best model parameters through cross-validation\n",
    "from sklearn.metrics import (  # For evaluating model performance\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, average_precision_score\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer  # For preprocessing multiple types of features\n",
    "from sklearn.preprocessing import StandardScaler  # For standardizing numerical data\n",
    "\n",
    "# Step 1: Load the data from CSV files\n",
    "data9 = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_bidmc_3948.csv')  # Training dataset\n",
    "data10 = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_bidmc_1664.csv')  # Testing dataset\n",
    "\n",
    "# Step 2: Define a vectorizer for text data preprocessing\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',  # Ignore common English words\n",
    "    ngram_range=(1, 3),  # Include single words, bigrams, and trigrams\n",
    "    max_features=3000  # Limit the number of features to the top 3000\n",
    ")\n",
    "\n",
    "# Step 3: Define a preprocessor for combining text and numerical data\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('vectorizer', vectorizer, 'report_text'),  # Apply text vectorization to the 'report_text' column\n",
    "    ('scaler', StandardScaler(), ['icd', 'med'])  # Scale numerical features 'icd' and 'med'\n",
    "], n_jobs=-1)\n",
    "\n",
    "# Step 4: Define a Random Forest classifier\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=50,  # Number of trees in the forest\n",
    "    max_depth=10,  # Maximum depth of each tree\n",
    "    class_weight=\"balanced\",  # Handle class imbalance\n",
    "    random_state=42,  # Ensure reproducibility\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Step 5: Combine preprocessing and the model into a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # Preprocess the data\n",
    "    ('classifier', model)  # Train the model\n",
    "])\n",
    "\n",
    "# Step 6: Define a parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 200],  # Vary the number of trees\n",
    "    'classifier__max_depth': [10, None, 20],  # Test different tree depths\n",
    "    'classifier__min_samples_split': [2, 5, 10],  # Minimum samples to split a node\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]  # Minimum samples per leaf node\n",
    "}\n",
    "\n",
    "# Step 7: Perform hyperparameter tuning using GridSearchCV\n",
    "grid_search4 = GridSearchCV(\n",
    "    pipeline,  # Pipeline to optimize\n",
    "    param_grid=param_grid,  # Parameter grid\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_jobs=-1  # Use all available processors\n",
    ")\n",
    "grid_search4.fit(data9[['icd', 'med', 'report_text']], data9['annot'])  # Train the model on the training data\n",
    "\n",
    "# Step 8: Output the best parameters and score from GridSearchCV\n",
    "print(\"Best Parameters: \", grid_search4.best_params_)\n",
    "print(\"Best Score: \", grid_search4.best_score_)\n",
    "\n",
    "# Step 9: Test the model on the test dataset\n",
    "X_test_data10 = data10[['icd', 'med', 'report_text']]  # Features from the test set\n",
    "y_test_data10 = data10['annot']  # Labels from the test set\n",
    "y_pred_data10 = grid_search4.predict(X_test_data10)  # Make predictions on the test set\n",
    "\n",
    "# Step 10: Evaluate model performance using bootstrapping\n",
    "n_iterations = 10  # Number of bootstrap iterations\n",
    "metrics_values = {  # Dictionary to store metrics for each iteration\n",
    "    'accuracy': [], 'precision': [], 'recall': [], 'f1': [],\n",
    "    'specificity': [], 'roc_auc': [], 'auprc': []\n",
    "}\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Sample the test data with replacement\n",
    "    indices = np.random.choice(len(X_test_data10), len(X_test_data10), replace=True)\n",
    "    X_sampled = X_test_data10.iloc[indices]  # Sampled features\n",
    "    y_sampled = y_test_data10.iloc[indices]  # Sampled labels\n",
    "\n",
    "    # Make predictions on the sampled data\n",
    "    y_pred_sampled = grid_search4.predict(X_sampled)\n",
    "    y_pred_prob_sampled = grid_search4.predict_proba(X_sampled)[:, 1]  # Predicted probabilities for the positive class\n",
    "\n",
    "    # Calculate specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_sampled, y_pred_sampled).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "\n",
    "    # Store calculated metrics\n",
    "    metrics_values['accuracy'].append(accuracy_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['precision'].append(precision_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['recall'].append(recall_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['f1'].append(f1_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['specificity'].append(specificity)\n",
    "    metrics_values['roc_auc'].append(roc_auc_score(y_sampled, y_pred_prob_sampled))\n",
    "    metrics_values['auprc'].append(average_precision_score(y_sampled, y_pred_prob_sampled))\n",
    "\n",
    "# Step 11: Calculate the mean and 95% confidence interval for each metric\n",
    "for metric, values in metrics_values.items():\n",
    "    mean_value = np.mean(values)  # Mean value of the metric\n",
    "    lower_band = np.percentile(values, 2.5)  # Lower bound of 95% confidence interval\n",
    "    upper_band = np.percentile(values, 97.5)  # Upper bound of 95% confidence interval\n",
    "\n",
    "    print(f\"{metric.capitalize()}: Mean={mean_value}, 95% CI=({lower_band}, {upper_band})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters:  {'classifier__max_depth': 20, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100}\n",
    "Best Score:  0.8890718903916189\n",
    "Accuracy: Mean=0.8220552884615383, 95% CI=(0.8156400240384616, 0.8317908653846153)\n",
    "Precision: Mean=0.9239405839191643, 95% CI=(0.9080767114494016, 0.9383105140790494)\n",
    "Recall: Mean=0.6697311911722641, 95% CI=(0.6487397195895668, 0.6919827669201887)\n",
    "F1: Mean=0.7764011612218064, 95% CI=(0.7621629541730918, 0.7944219648926621)\n",
    "Specificity: Mean=0.9526991896825967, 95% CI=(0.94106142936084, 0.9608481107222899)\n",
    "Roc_auc: Mean=0.9413359556233969, 95% CI=(0.9300670303544616, 0.946915722355956)\n",
    "Auprc: Mean=0.9013509970938347, 95% CI=(0.8780906732584564, 0.9233003272627427)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMX8BH1Afcmy"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsUnb7OTe7q7"
   },
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 124259,
     "status": "ok",
     "timestamp": 1737777861140,
     "user": {
      "displayName": "Ruoqi Wei",
      "userId": "06248467831337262349"
     },
     "user_tz": 300
    },
    "id": "gJJhkY3OnWud",
    "outputId": "9228bd28-3fa6-44c4-be74-d783233d7db1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier__alpha': 0.1}\n",
      "Best Score:  0.7594041488184049\n",
      "Accuracy: Mean=0.6150841346153845, 95% CI=(0.6003004807692308, 0.6306640625)\n",
      "Precision: Mean=0.5811091435381704, 95% CI=(0.5584917272080078, 0.6175252723785309)\n",
      "Recall: Mean=0.6204689002111665, 95% CI=(0.5999882237355656, 0.6405943650377909)\n",
      "F1: Mean=0.6000091860930791, 95% CI=(0.5823706771114112, 0.6275441710399119)\n",
      "Specificity: Mean=0.6102356476928656, 95% CI=(0.586841814821466, 0.6302638295952626)\n",
      "Roc_auc: Mean=0.6413962575594194, 95% CI=(0.6283590832835436, 0.6559458587480677)\n",
      "Auprc: Mean=0.5731513227041299, 95% CI=(0.5520553530710124, 0.6043068560895215)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for data handling, machine learning, and performance evaluation\n",
    "import pandas as pd  # For working with tabular data\n",
    "import numpy as np  # For numerical operations\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # For text feature extraction\n",
    "from sklearn.naive_bayes import BernoulliNB  # Naive Bayes classifier (Bernoulli for binary features)\n",
    "from sklearn.pipeline import Pipeline  # For combining preprocessing and modeling into a single workflow\n",
    "from sklearn.model_selection import GridSearchCV  # For hyperparameter optimization\n",
    "from sklearn.metrics import (  # For evaluating model performance\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, average_precision_score\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer  # For preprocessing text and numerical data\n",
    "from sklearn.preprocessing import StandardScaler  # For scaling numerical data\n",
    "\n",
    "# Step 1: Load datasets\n",
    "data9 = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_bidmc_3948.csv')  # Training dataset\n",
    "data10 = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_bidmc_1664.csv')  # Test dataset\n",
    "\n",
    "# Step 2: Define the text vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',  # Exclude common English words\n",
    "    ngram_range=(1, 3),  # Include unigrams, bigrams, and trigrams\n",
    "    max_features=3000,  # Limit features to the top 3000 by importance\n",
    "    norm='l1'  # Normalize to make values non-negative\n",
    ")\n",
    "\n",
    "# Step 3: Define preprocessing for both text and numerical features\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('vectorizer', vectorizer, 'report_text'),  # Apply text vectorizer to 'report_text'\n",
    "    ('scaler', StandardScaler(), ['icd', 'med'])  # Scale numerical features ('icd', 'med')\n",
    "], sparse_threshold=0, n_jobs=-1)  # Ensure output is a dense matrix (not sparse)\n",
    "\n",
    "# Step 4: Define the Naive Bayes classifier\n",
    "model = BernoulliNB()  # Suitable for binary or indicator features\n",
    "\n",
    "# Step 5: Combine preprocessing and model into a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # Preprocessing step\n",
    "    ('classifier', model)  # Classification step\n",
    "])\n",
    "\n",
    "# Step 6: Define hyperparameter grid for optimization\n",
    "param_grid = {\n",
    "    'classifier__alpha': [0.01, 0.1, 1.0, 10.0]  # Regularization parameter for Naive Bayes\n",
    "}\n",
    "\n",
    "# Step 7: Use GridSearchCV for hyperparameter tuning\n",
    "grid_search4 = GridSearchCV(\n",
    "    pipeline,  # The pipeline to optimize\n",
    "    param_grid=param_grid,  # Hyperparameter grid\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_jobs=-1  # Utilize all available CPU cores\n",
    ")\n",
    "grid_search4.fit(data9[['icd', 'med', 'report_text']], data9['annot'])  # Train the model\n",
    "\n",
    "# Step 8: Output the best parameters and corresponding score\n",
    "print(\"Best Parameters: \", grid_search4.best_params_)\n",
    "print(\"Best Score: \", grid_search4.best_score_)\n",
    "\n",
    "# Step 9: Predict on test data\n",
    "X_test_data10 = data10[['icd', 'med', 'report_text']]  # Features from test dataset\n",
    "y_test_data10 = data10['annot']  # Labels from test dataset\n",
    "y_pred_data10 = grid_search4.predict(X_test_data10)  # Predictions\n",
    "y_pred_prob_data10 = grid_search4.predict_proba(X_test_data10)[:, 1]  # Probabilities for positive class\n",
    "\n",
    "# Step 10: Evaluate metrics using bootstrapping\n",
    "n_iterations = 10  # Number of bootstrap samples\n",
    "metrics_values = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'specificity': [], 'roc_auc': [], 'auprc': []}\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Resample test data with replacement\n",
    "    indices = np.random.choice(len(X_test_data10), len(X_test_data10), replace=True)\n",
    "    X_sampled = X_test_data10.iloc[indices]\n",
    "    y_sampled = y_test_data10.iloc[indices]\n",
    "\n",
    "    # Make predictions on the resampled data\n",
    "    y_pred_sampled = grid_search4.best_estimator_.predict(X_sampled)\n",
    "    y_pred_prob_sampled = grid_search4.best_estimator_.predict_proba(X_sampled)[:, 1]\n",
    "\n",
    "    # Calculate confusion matrix and specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_sampled, y_pred_sampled).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "\n",
    "    # Store evaluation metrics\n",
    "    metrics_values['accuracy'].append(accuracy_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['precision'].append(precision_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['recall'].append(recall_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['f1'].append(f1_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['specificity'].append(specificity)\n",
    "    metrics_values['roc_auc'].append(roc_auc_score(y_sampled, y_pred_prob_sampled))\n",
    "    metrics_values['auprc'].append(average_precision_score(y_sampled, y_pred_prob_sampled))\n",
    "\n",
    "# Step 11: Calculate mean and 95% confidence intervals for each metric\n",
    "for metric, values in metrics_values.items():\n",
    "    mean_value = np.mean(values)\n",
    "    lower_band = np.percentile(values, 2.5)\n",
    "    upper_band = np.percentile(values, 97.5)\n",
    "\n",
    "    print(f\"{metric.capitalize()}: Mean={mean_value}, 95% CI=({lower_band}, {upper_band})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters:  {'classifier__alpha': 10.0}\n",
    "Best Score:  0.7594019027450225\n",
    "Accuracy: Mean=0.6019831730769232, 95% CI=(0.5893629807692308, 0.615850360576923)\n",
    "Precision: Mean=0.5708670822863977, 95% CI=(0.5555554262844379, 0.6017076752239154)\n",
    "Recall: Mean=0.5895687241376917, 95% CI=(0.5717752270574469, 0.6043827865633813)\n",
    "F1: Mean=0.5798908941338062, 95% CI=(0.5698344613614915, 0.590695557574248)\n",
    "Specificity: Mean=0.612975671861568, 95% CI=(0.5940088395126994, 0.6413795649018011)\n",
    "Roc_auc: Mean=0.6268913992151288, 95% CI=(0.6127829991337536, 0.6447232939760171)\n",
    "Auprc: Mean=0.5636663277837618, 95% CI=(0.5443730018353451, 0.5914388665507918)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eK2CGOOSfwD5"
   },
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 629411,
     "status": "ok",
     "timestamp": 1737778638152,
     "user": {
      "displayName": "Ruoqi Wei",
      "userId": "06248467831337262349"
     },
     "user_tz": 300
    },
    "id": "W9bpV5CXovrr",
    "outputId": "303efe8b-9113-482d-bb41-f8391f5232fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier__activation': 'tanh', 'classifier__alpha': 0.001, 'classifier__hidden_layer_sizes': (100,), 'classifier__learning_rate': 'constant', 'classifier__solver': 'adam'}\n",
      "Best Score:  0.8255019171840658\n",
      "Accuracy: Mean=0.7959735576923077, 95% CI=(0.7854717548076924, 0.8047475961538462)\n",
      "Precision: Mean=0.8658851926664337, 95% CI=(0.8339552207345452, 0.8823349520531104)\n",
      "Recall: Mean=0.6658160627228595, 95% CI=(0.6443331102690677, 0.6884194181104936)\n",
      "F1: Mean=0.7526593004338398, 95% CI=(0.7361451973797446, 0.7697121571150587)\n",
      "Specificity: Mean=0.9097651234457265, 95% CI=(0.8911432323756404, 0.9251365982837222)\n",
      "Roc_auc: Mean=0.8821861441445712, 95% CI=(0.8677481027791552, 0.8909715485188199)\n",
      "Auprc: Mean=0.8770937185026456, 95% CI=(0.8465984478810411, 0.88986911380543)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd  # For data manipulation\n",
    "import numpy as np  # For numerical operations\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # For converting text data into numerical features\n",
    "from sklearn.neural_network import MLPClassifier  # Multi-layer Perceptron (MLP) classifier for classification tasks\n",
    "from sklearn.pipeline import Pipeline  # For creating a workflow combining preprocessing and modeling\n",
    "from sklearn.model_selection import GridSearchCV  # For hyperparameter tuning using cross-validation\n",
    "from sklearn.metrics import (  # For evaluating model performance\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer  # For preprocessing both text and numerical data\n",
    "from sklearn.preprocessing import StandardScaler  # For standardizing numerical features\n",
    "\n",
    "# Load the datasets\n",
    "data9 = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_bidmc_3948.csv')  # Training dataset\n",
    "data10 = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_bidmc_1664.csv')  # Testing dataset\n",
    "\n",
    "# Define a TF-IDF vectorizer for text preprocessing\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',  # Exclude common English words\n",
    "    ngram_range=(1, 3),  # Consider unigrams, bigrams, and trigrams\n",
    "    max_features=3000  # Limit the number of features to the top 3000\n",
    ")\n",
    "\n",
    "# Define preprocessing for both text and numerical data\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('vectorizer', vectorizer, 'report_text'),  # Apply the TF-IDF vectorizer to the 'report_text' column\n",
    "    ('scaler', StandardScaler(), ['icd', 'med'])  # Standardize the numerical columns 'icd' and 'med'\n",
    "], sparse_threshold=0, n_jobs=-1)  # Ensure dense output for compatibility with the classifier\n",
    "\n",
    "# Define an MLPClassifier for classification\n",
    "model = MLPClassifier(\n",
    "    random_state=42,  # Set random seed for reproducibility\n",
    "    max_iter=200  # Maximum number of iterations for training\n",
    ")\n",
    "\n",
    "# Combine preprocessing and modeling into a single pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # Preprocess the data\n",
    "    ('classifier', model)  # Apply the classifier\n",
    "])\n",
    "\n",
    "# Define the hyperparameter grid for tuning the MLP classifier\n",
    "param_grid = {\n",
    "    'classifier__hidden_layer_sizes': [(100,), (50, 50), (100, 50)],  # Network architectures to test\n",
    "    'classifier__activation': ['relu', 'tanh'],  # Activation functions to test\n",
    "    'classifier__solver': ['adam'],  # Optimization algorithm\n",
    "    'classifier__learning_rate': ['constant', 'adaptive'],  # Learning rate strategies\n",
    "    'classifier__alpha': [0.0001, 0.001]  # Regularization strength (L2 penalty)\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning with GridSearchCV\n",
    "grid_search4 = GridSearchCV(\n",
    "    pipeline,  # Pipeline to optimize\n",
    "    param_grid=param_grid,  # Hyperparameter grid\n",
    "    cv=5,  # Use 5-fold cross-validation\n",
    "    n_jobs=-1  # Use all available CPU cores\n",
    ")\n",
    "grid_search4.fit(data9[['icd', 'med', 'report_text']], data9['annot'])  # Train the model on the training data\n",
    "\n",
    "# Output the best hyperparameters and corresponding cross-validation score\n",
    "print(\"Best Parameters: \", grid_search4.best_params_)\n",
    "print(\"Best Score: \", grid_search4.best_score_)\n",
    "\n",
    "# Use the trained model to make predictions on the test dataset\n",
    "X_test_data10 = data10[['icd', 'med', 'report_text']]  # Extract test features\n",
    "y_test_data10 = data10['annot']  # Extract test labels\n",
    "y_pred_data10 = grid_search4.predict(X_test_data10)  # Make predictions\n",
    "y_pred_prob_data10 = grid_search4.predict_proba(X_test_data10)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "# Evaluate the model using bootstrapping\n",
    "n_iterations = 10  # Number of bootstrap iterations\n",
    "metrics_values = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'specificity': [], 'roc_auc': [], 'auprc': []}\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Resample the test data with replacement\n",
    "    indices = np.random.choice(len(X_test_data10), len(X_test_data10), replace=True)\n",
    "    X_sampled = X_test_data10.iloc[indices]  # Resampled test features\n",
    "    y_sampled = y_test_data10.iloc[indices]  # Resampled test labels\n",
    "\n",
    "    # Make predictions on the resampled data\n",
    "    y_pred_sampled = grid_search4.best_estimator_.predict(X_sampled)\n",
    "    y_pred_prob_sampled = grid_search4.best_estimator_.predict_proba(X_sampled)[:, 1]\n",
    "\n",
    "    # Calculate confusion matrix and specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_sampled, y_pred_sampled).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "\n",
    "    # Store the evaluation metrics\n",
    "    metrics_values['accuracy'].append(accuracy_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['precision'].append(precision_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['recall'].append(recall_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['f1'].append(f1_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['specificity'].append(specificity)\n",
    "    metrics_values['roc_auc'].append(roc_auc_score(y_sampled, y_pred_prob_sampled))\n",
    "    metrics_values['auprc'].append(average_precision_score(y_sampled, y_pred_prob_sampled))\n",
    "\n",
    "# Calculate and print the mean and 95% confidence intervals for each metric\n",
    "for metric, values in metrics_values.items():\n",
    "    mean_value = np.mean(values)  # Mean value of the metric\n",
    "    lower_band = np.percentile(values, 2.5)  # Lower bound of the 95% confidence interval\n",
    "    upper_band = np.percentile(values, 97.5)  # Upper bound of the 95% confidence interval\n",
    "\n",
    "    print(f\"{metric.capitalize()}: Mean={mean_value}, 95% CI=({lower_band}, {upper_band})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters:  {'classifier__activation': 'tanh', 'classifier__alpha': 0.001, 'classifier__hidden_layer_sizes': (100,), 'classifier__learning_rate': 'constant', 'classifier__solver': 'adam'}\n",
    "Best Score:  0.8247427443808057\n",
    "Accuracy: Mean=0.8024038461538462, 95% CI=(0.7956730769230769, 0.8152794471153846)\n",
    "Precision: Mean=0.8778107180430034, 95% CI=(0.8525246423766508, 0.9009188418841885)\n",
    "Recall: Mean=0.6778622178207565, 95% CI=(0.6664623410193493, 0.696118538108178)\n",
    "F1: Mean=0.764903747393255, 95% CI=(0.7516295025728988, 0.7779750548627472)\n",
    "Specificity: Mean=0.9149055840295353, 95% CI=(0.9012071938901207, 0.9315183585546821)\n",
    "Roc_auc: Mean=0.882651963169548, 95% CI=(0.8676672446863012, 0.8916546078941331)\n",
    "Auprc: Mean=0.8810523197871515, 95% CI=(0.8611588246415243, 0.8960769652776367)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0D5hX81gFVY"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 270688,
     "status": "ok",
     "timestamp": 1737779363967,
     "user": {
      "displayName": "Ruoqi Wei",
      "userId": "06248467831337262349"
     },
     "user_tz": 300
    },
    "id": "rJ1ACNeStJNb",
    "outputId": "e98f5a65-fbad-45f3-af2b-41f6a0e52a73"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n",
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n",
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n",
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n",
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n",
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n",
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n",
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n",
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n",
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n",
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n",
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n",
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n",
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n",
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n",
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n",
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n",
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n",
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n",
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n",
      "/home/niels/miniforge3/envs/playground/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier__C': 10.0, 'classifier__penalty': 'l1'}\n",
      "Best Score:  0.9032725289181947\n",
      "Accuracy: Mean=0.9147836538461538, 95% CI=(0.9036508413461538, 0.9253455528846154)\n",
      "Precision: Mean=0.9269922475500639, 95% CI=(0.9145133786733259, 0.93889917151419)\n",
      "Recall: Mean=0.8861268045542816, 95% CI=(0.8696290984667979, 0.9059884075655887)\n",
      "F1: Mean=0.9060557923702705, 95% CI=(0.8952551674495561, 0.9197443237138295)\n",
      "Specificity: Mean=0.9395262129625529, 95% CI=(0.9256916458398418, 0.9499064647398154)\n",
      "Roc_auc: Mean=0.9685777480247992, 95% CI=(0.964485710029028, 0.9741742940236007)\n",
      "Auprc: Mean=0.9691984571858736, 95% CI=(0.9651149537049218, 0.9737572526108763)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # For handling tabular data\n",
    "import numpy as np  # For numerical operations\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # For converting text into numerical features using TF-IDF\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic regression classifier\n",
    "from sklearn.pipeline import Pipeline  # For creating a workflow combining preprocessing and modeling\n",
    "from sklearn.model_selection import GridSearchCV  # For hyperparameter tuning using cross-validation\n",
    "from sklearn.metrics import (  # For evaluating model performance\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer  # For preprocessing multiple types of data (e.g., text and numerical)\n",
    "from sklearn.preprocessing import StandardScaler  # For standardizing numerical data\n",
    "\n",
    "# Step 1: Load data\n",
    "data9 = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_bidmc_3948.csv')  # Training dataset\n",
    "data10 = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_bidmc_1664.csv')  # Testing dataset\n",
    "\n",
    "# Step 2: Define a TF-IDF vectorizer for text data\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',  # Remove common English stop words\n",
    "    ngram_range=(1, 3),  # Include unigrams, bigrams, and trigrams\n",
    "    min_df=2,  # Ignore terms appearing in fewer than 2 documents\n",
    "    max_df=0.9  # Ignore terms appearing in more than 90% of documents\n",
    ")\n",
    "\n",
    "# Step 3: Define class weights for handling class imbalance\n",
    "class_weights = {0: 1.0, 1: 3.0}  # Assign higher weight to the positive class (1)\n",
    "\n",
    "# Step 4: Define the logistic regression model\n",
    "model = LogisticRegression(\n",
    "    solver='liblinear',  # Use 'liblinear' solver (suitable for small datasets and L1 regularization)\n",
    "    class_weight=class_weights,  # Incorporate class weights\n",
    "    n_jobs=-1,  # Ensure reproducibility\n",
    ")\n",
    "\n",
    "# Step 5: Define a preprocessor for both text and numerical data\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('vectorizer', vectorizer, 'report_text'),  # Apply the TF-IDF vectorizer to the 'report_text' column\n",
    "    ('scaler', StandardScaler(), ['icd', 'med'])  # Standardize the numerical columns 'icd' and 'med'\n",
    "], n_jobs=-1)\n",
    "\n",
    "# Step 6: Create a pipeline combining preprocessing and logistic regression\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # Preprocessing step\n",
    "    ('classifier', model)  # Logistic regression classifier\n",
    "])\n",
    "\n",
    "# Step 7: Define a hyperparameter grid for tuning the logistic regression model\n",
    "param_grid = {\n",
    "    'classifier__penalty': ['l1'],  # Use L1 regularization (Lasso)\n",
    "    'classifier__C': [0.01, 0.1, 1.0, 10.0]  # Regularization strength\n",
    "}\n",
    "\n",
    "# Step 8: Perform hyperparameter tuning with GridSearchCV\n",
    "grid_search4 = GridSearchCV(\n",
    "    pipeline,  # Pipeline to optimize\n",
    "    param_grid=param_grid,  # Hyperparameter grid\n",
    "    cv=5  # 5-fold cross-validation\n",
    ")\n",
    "grid_search4.fit(data9[['icd', 'med', 'report_text']], data9['annot'])  # Train the model on the training dataset\n",
    "\n",
    "# Step 9: Output the best hyperparameters and corresponding score\n",
    "print(\"Best Parameters: \", grid_search4.best_params_)\n",
    "print(\"Best Score: \", grid_search4.best_score_)\n",
    "\n",
    "# Step 10: Make predictions on the test dataset\n",
    "X_test_data10 = data10[['icd', 'med', 'report_text']]  # Extract features from the test dataset\n",
    "y_test_data10 = data10['annot']  # Extract labels from the test dataset\n",
    "y_pred_data10 = grid_search4.predict(X_test_data10)  # Predict labels for the test dataset\n",
    "\n",
    "# Step 11: Initialize metrics dictionary for bootstrapping\n",
    "n_iterations = 10  # Number of bootstrap iterations\n",
    "metrics_values = {  # Store evaluation metrics\n",
    "    'accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': [],\n",
    "    'specificity': [],\n",
    "    'roc_auc': [],\n",
    "    'auprc': []\n",
    "}\n",
    "\n",
    "# Step 12: Perform bootstrapping to evaluate metrics\n",
    "for _ in range(n_iterations):\n",
    "    # Sample test data with replacement\n",
    "    indices = np.random.choice(len(X_test_data10), len(X_test_data10), replace=True)\n",
    "    X_sampled = X_test_data10.iloc[indices]  # Resampled test features\n",
    "    y_sampled = y_test_data10.iloc[indices]  # Resampled test labels\n",
    "\n",
    "    # Predict labels and probabilities for the resampled data\n",
    "    y_pred_sampled = grid_search4.predict(X_sampled)\n",
    "    y_pred_prob_sampled = grid_search4.predict_proba(X_sampled)[:, 1]  # Probability of the positive class\n",
    "\n",
    "    # Calculate confusion matrix and specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_sampled, y_pred_sampled).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0  # True negative rate\n",
    "\n",
    "    # Append calculated metrics to the dictionary\n",
    "    metrics_values['accuracy'].append(accuracy_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['precision'].append(precision_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['recall'].append(recall_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['f1'].append(f1_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['specificity'].append(specificity)\n",
    "    metrics_values['roc_auc'].append(roc_auc_score(y_sampled, y_pred_prob_sampled))\n",
    "    metrics_values['auprc'].append(average_precision_score(y_sampled, y_pred_prob_sampled))\n",
    "\n",
    "# Step 13: Calculate mean and confidence intervals for each metric\n",
    "for metric, values in metrics_values.items():\n",
    "    mean_value = np.mean(values)  # Calculate mean of the metric\n",
    "    lower_band = np.percentile(values, 2.5)  # 2.5th percentile (lower bound)\n",
    "    upper_band = np.percentile(values, 97.5)  # 97.5th percentile (upper bound)\n",
    "\n",
    "    # Print the mean and confidence interval for the metric\n",
    "    print(f\"{metric.capitalize()}: Mean={mean_value}, 95% CI=({lower_band}, {upper_band})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters:  {'classifier__C': 10.0, 'classifier__penalty': 'l1'}\n",
    "Best Score:  0.9032725289181947\n",
    "Accuracy: Mean=0.911778846153846, 95% CI=(0.8979717548076923, 0.9196664663461539)\n",
    "Precision: Mean=0.9237657028521212, 95% CI=(0.9127599616450032, 0.9401544893310957)\n",
    "Recall: Mean=0.8842612220300854, 95% CI=(0.8644202758908641, 0.9010599211563732)\n",
    "F1: Mean=0.9035477237691338, 95% CI=(0.8879237372156347, 0.9138079434184725)\n",
    "Specificity: Mean=0.9359959705818681, 95% CI=(0.9222324805339266, 0.9462493721848502)\n",
    "Roc_auc: Mean=0.9685768369699211, 95% CI=(0.9568631442155783, 0.9764631731839605)\n",
    "Auprc: Mean=0.9693851581842349, 95% CI=(0.9587637922673002, 0.9782104340774328)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPuFbH6x6qin9TzT94BsLpU",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1jbyYBXiLPBa7M3VF7TRqs-QYVLq-i-Oy",
     "timestamp": 1737859220234
    }
   ]
  },
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
