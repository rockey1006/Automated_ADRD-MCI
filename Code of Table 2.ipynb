{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53CQqeX00ydO"
   },
   "source": [
    "## 1.Table 2: Average performance  and [95% confidence intervals] for logistic regression models using ICD Only, Med Only, Note Only, and ICD+MED+Note in the MGH+BIDMC training sets and MGH+BIDMC testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeor_z8A3lPx"
   },
   "source": [
    "### 1.1  Note **Only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 410391,
     "status": "ok",
     "timestamp": 1737648117239,
     "user": {
      "displayName": "Ruoqi Wei",
      "userId": "06248467831337262349"
     },
     "user_tz": 300
    },
    "id": "pjd1QsI81Gwe",
    "outputId": "9bddec2a-2b8c-4650-b50d-f81940b7afc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier__C': 1.0, 'classifier__penalty': 'l1'}\n",
      "Best Score:  0.915924981149027\n",
      "Accuracy: Mean=0.8936899038461539, 95% CI=(0.8818209134615385, 0.9088942307692308)\n",
      "Precision: Mean=0.8886770498364014, 95% CI=(0.8731285665127749, 0.9071458351201197)\n",
      "Recall: Mean=0.8855674029265812, 95% CI=(0.8687843481230788, 0.9054281655844156)\n",
      "F1: Mean=0.887080392028369, 95% CI=(0.8728417899929528, 0.9042928173835972)\n",
      "Specificity: Mean=0.900875676281894, 95% CI=(0.8849602957105012, 0.9145477658135664)\n",
      "Roc_auc: Mean=0.9505194139312042, 95% CI=(0.942088730768383, 0.9600750052684119)\n",
      "Auprc: Mean=0.9495666842376862, 95% CI=(0.941345046239817, 0.9581640617200945)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "np.random.seed(2025)\n",
    "\n",
    "# Load training and test datasets\n",
    "train_data = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_bidmc_3948.csv')  # Training dataset\n",
    "test_data = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_bidmc_1664.csv')  # Test dataset\n",
    "\n",
    "# Define the text feature extractor using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))  # Extract unigrams, bigrams, and trigrams\n",
    "\n",
    "# Define class weights to handle imbalanced classes\n",
    "class_weight_dict = {0: 1.0, 1: 3.0}  # Assign higher weight to the positive class\n",
    "\n",
    "# Define the classification model\n",
    "logistic_model = LogisticRegression(solver='liblinear', class_weight=class_weight_dict, random_state=2025)  # Logistic regression with weighted classes\n",
    "\n",
    "# Define the preprocessor for feature extraction\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('tfidf', tfidf_vectorizer, 'report_text'),  # Apply TF-IDF to the 'report_text' column\n",
    "], n_jobs=-1)\n",
    "\n",
    "# Create a pipeline for preprocessing and classification\n",
    "classification_pipeline = Pipeline([\n",
    "    ('preprocessor', data_preprocessor),\n",
    "    ('classifier', logistic_model)\n",
    "])\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "hyperparameter_grid = {\n",
    "    'classifier__penalty': ['l1'],  # L1 regularization\n",
    "    'classifier__C': [0.01, 0.1, 1.0, 10.0]  # Regularization strength\n",
    "}\n",
    "\n",
    "# Perform grid search for hyperparameter optimization\n",
    "grid_search = GridSearchCV(classification_pipeline, param_grid=hyperparameter_grid, cv=5, n_jobs=-1)  # 5-fold cross-validation\n",
    "grid_search.fit(train_data[['report_text']], train_data['annot'])  # Fit the model on the training data\n",
    "\n",
    "# Display the best parameters and the corresponding score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_features = test_data[['report_text']]  # Features from the test set\n",
    "test_labels = test_data['annot']  # Labels from the test set\n",
    "test_predictions = grid_search.predict(test_features)  # Predict labels for the test set\n",
    "\n",
    "# Perform bootstrapping on the test set to calculate evaluation metrics\n",
    "bootstrap_iterations = 10  # Number of bootstrap iterations\n",
    "evaluation_metrics = {\n",
    "    'accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': [],\n",
    "    'specificity': [],\n",
    "    'roc_auc': [],\n",
    "    'auprc': []\n",
    "}\n",
    "\n",
    "for _ in range(bootstrap_iterations):\n",
    "    # Sample test set with replacement\n",
    "    sampled_indices = np.random.choice(len(test_features), len(test_features), replace=True)\n",
    "    sampled_features = test_features.iloc[sampled_indices]\n",
    "    sampled_labels = test_labels.iloc[sampled_indices]\n",
    "\n",
    "    # Predict on the sampled data\n",
    "    sampled_predictions = grid_search.predict(sampled_features)\n",
    "    sampled_probabilities = grid_search.predict_proba(sampled_features)[:, 1]  # Probability for the positive class\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    tn, fp, fn, tp = confusion_matrix(sampled_labels, sampled_predictions).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "\n",
    "    evaluation_metrics['accuracy'].append(accuracy_score(sampled_labels, sampled_predictions))\n",
    "    evaluation_metrics['precision'].append(precision_score(sampled_labels, sampled_predictions))\n",
    "    evaluation_metrics['recall'].append(recall_score(sampled_labels, sampled_predictions))\n",
    "    evaluation_metrics['f1'].append(f1_score(sampled_labels, sampled_predictions))\n",
    "    evaluation_metrics['specificity'].append(specificity)\n",
    "    evaluation_metrics['roc_auc'].append(roc_auc_score(sampled_labels, sampled_probabilities))\n",
    "    evaluation_metrics['auprc'].append(average_precision_score(sampled_labels, sampled_probabilities))\n",
    "\n",
    "# Calculate the mean and 95% confidence interval for each metric\n",
    "for metric_name, metric_values in evaluation_metrics.items():\n",
    "    mean_value = np.mean(metric_values)\n",
    "    lower_bound = np.percentile(metric_values, 2.5)  # 2.5th percentile\n",
    "    upper_bound = np.percentile(metric_values, 97.5)  # 97.5th percentile\n",
    "\n",
    "    print(f\"{metric_name.capitalize()}: Mean={mean_value}, 95% CI=({lower_bound}, {upper_bound})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters:  {'classifier__C': 1.0, 'classifier__penalty': 'l1'}\n",
    "Best Score:  0.915924981149027\n",
    "Accuracy: Mean=0.8903846153846156, 95% CI=(0.8792067307692307, 0.9003004807692307)\n",
    "Precision: Mean=0.8878943770543042, 95% CI=(0.8690930397933417, 0.9027105702074429)\n",
    "Recall: Mean=0.8783526438224714, 95% CI=(0.8684446424827008, 0.8997620545864902)\n",
    "F1: Mean=0.8830274360263596, 95% CI=(0.8718387280531478, 0.891931275914624)\n",
    "Specificity: Mean=0.9011045991650632, 95% CI=(0.8833159103394945, 0.919364645632073)\n",
    "Roc_auc: Mean=0.9467935690783327, 95% CI=(0.9407033765153447, 0.9520673768926069)\n",
    "Auprc: Mean=0.9445529954909692, 95% CI=(0.9367879638540018, 0.949813157060619)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Urz5UeOe8mFs"
   },
   "source": [
    "### 1.2 ICD+MED+Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 430552,
     "status": "ok",
     "timestamp": 1737649578206,
     "user": {
      "displayName": "Ruoqi Wei",
      "userId": "06248467831337262349"
     },
     "user_tz": 300
    },
    "id": "AzRwWaRW9GGp",
    "outputId": "eb60e34a-a911-463c-a9f2-863ca7c000d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier__C': 1.0, 'classifier__penalty': 'l1'}\n",
      "Best Score:  0.8941525083826667\n",
      "Accuracy: Mean=0.8917, 95% CI=(0.8834, 0.9032)\n",
      "Precision: Mean=0.8885, 95% CI=(0.8752, 0.8964)\n",
      "Recall: Mean=0.8774, 95% CI=(0.8565, 0.8979)\n",
      "F1: Mean=0.8829, 95% CI=(0.8739, 0.8948)\n",
      "Specificity: Mean=0.9040, 95% CI=(0.8856, 0.9150)\n",
      "Roc_auc: Mean=0.9489, 95% CI=(0.9421, 0.9545)\n",
      "Auprc: Mean=0.9470, 95% CI=(0.9388, 0.9531)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load training and test datasets\n",
    "train_data = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_bidmc_3948.csv')\n",
    "test_data = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_bidmc_1664.csv')\n",
    "\n",
    "# Define the text feature extractor using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))\n",
    "\n",
    "# Define class weights to handle imbalanced classes\n",
    "class_weights = {0: 1.0, 1: 3.0}\n",
    "\n",
    "# Define the classification model\n",
    "logistic_model = LogisticRegression(solver='liblinear', class_weight=class_weights, random_state=2025)  # Logistic regression with weighted classes\n",
    "\n",
    "# Define the preprocessor for feature extraction and scaling\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('text_vectorizer', tfidf_vectorizer, 'report_text'),  # Apply TF-IDF to 'report_text'\n",
    "    ('scaler', StandardScaler(), ['icd', 'med'])  # Scale numerical features 'icd' and 'med'\n",
    "], n_jobs=-1)\n",
    "\n",
    "# Create a pipeline for preprocessing and classification\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', logistic_model)\n",
    "])\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'classifier__penalty': ['l1'],  # L1 regularization\n",
    "    'classifier__C': [0.01, 0.1, 1.0, 10.0]  # Regularization strength\n",
    "}\n",
    "\n",
    "# Perform grid search for hyperparameter optimization\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, n_jobs=-1)  # 5-fold cross-validation\n",
    "grid_search.fit(train_data[['icd', 'med', 'report_text']], train_data['annot'])  # Fit on training data\n",
    "\n",
    "# Display the best parameters and score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "X_test = test_data[['icd', 'med', 'report_text']]\n",
    "y_test = test_data['annot']\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Perform bootstrapping on the test set to calculate evaluation metrics\n",
    "n_iterations = 10  # Number of bootstrap iterations\n",
    "metrics_values = {\n",
    "    'accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': [],\n",
    "    'specificity': [],\n",
    "    'roc_auc': [],\n",
    "    'auprc': []\n",
    "}\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Sample test set with replacement\n",
    "    sampled_indices = np.random.choice(len(X_test), len(X_test), replace=True)\n",
    "    X_sampled = X_test.iloc[sampled_indices]\n",
    "    y_sampled = y_test.iloc[sampled_indices]\n",
    "\n",
    "    # Predict on the sampled data\n",
    "    y_pred_sampled = grid_search.predict(X_sampled)\n",
    "    y_pred_prob_sampled = grid_search.predict_proba(X_sampled)[:, 1]  # Probability of the positive class\n",
    "\n",
    "    # Calculate metrics\n",
    "    tn, fp, fn, tp = confusion_matrix(y_sampled, y_pred_sampled).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "\n",
    "    metrics_values['accuracy'].append(accuracy_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['precision'].append(precision_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['recall'].append(recall_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['f1'].append(f1_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['specificity'].append(specificity)\n",
    "    metrics_values['roc_auc'].append(roc_auc_score(y_sampled, y_pred_prob_sampled))\n",
    "    metrics_values['auprc'].append(average_precision_score(y_sampled, y_pred_prob_sampled))\n",
    "\n",
    "# Calculate mean and 95% confidence intervals for each metric\n",
    "for metric, values in metrics_values.items():\n",
    "    mean_value = np.mean(values)\n",
    "    lower_bound = np.percentile(values, 2.5)\n",
    "    upper_bound = np.percentile(values, 97.5)\n",
    "\n",
    "    print(f\"{metric.capitalize()}: Mean={mean_value:.4f}, 95% CI=({lower_bound:.4f}, {upper_bound:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters:  {'classifier__C': 1.0, 'classifier__penalty': 'l1'}\n",
    "Best Score:  0.8941525083826667\n",
    "Accuracy: Mean=0.8882, 95% CI=(0.8797, 0.8982)\n",
    "Precision: Mean=0.8847, 95% CI=(0.8643, 0.8941)\n",
    "Recall: Mean=0.8744, 95% CI=(0.8579, 0.8934)\n",
    "F1: Mean=0.8795, 95% CI=(0.8690, 0.8923)\n",
    "Specificity: Mean=0.9003, 95% CI=(0.8853, 0.9106)\n",
    "Roc_auc: Mean=0.9503, 95% CI=(0.9411, 0.9576)\n",
    "Auprc: Mean=0.9493, 95% CI=(0.9440, 0.9554)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEvCDvb8-mSi"
   },
   "source": [
    "### 1.3 ICD Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 699,
     "status": "ok",
     "timestamp": 1737649684608,
     "user": {
      "displayName": "Ruoqi Wei",
      "userId": "06248467831337262349"
     },
     "user_tz": 300
    },
    "id": "qbxVNOyq9u0F",
    "outputId": "7e5498b6-e744-4e45-fbba-859d5bbd4e9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy CI: ConfidenceInterval(low=np.float64(0.5249661559668469), high=np.float64(0.5526510480887794))\n",
      "F1-score CI: ConfidenceInterval(low=np.float64(0.5879624029643268), high=np.float64(0.6195655503187343))\n",
      "Recall CI: ConfidenceInterval(low=np.float64(0.6896647701173418), high=np.float64(0.7190944242192289))\n",
      "Precision CI: ConfidenceInterval(low=np.float64(0.511418720159229), high=np.float64(0.545396658106908))\n",
      "AUROC CI: ConfidenceInterval(low=np.float64(0.5269855608153995), high=np.float64(0.549233646724235))\n",
      "AUPRC CI: ConfidenceInterval(low=np.float64(0.6763990425178069), high=np.float64(0.7032895434829793))\n",
      "Specificity CI: ConfidenceInterval(low=np.float64(0.3542103681354118), high=np.float64(0.3829318379882789))\n",
      "Average Accuracy: 0.536861686296186\n",
      "Average F1-score: 0.602855387552034\n",
      "Average Recall: 0.7039825385220351\n",
      "Average Precision: 0.5273178096220179\n",
      "Average AUROC: 0.5365554399448353\n",
      "Average AUPRC: 0.689491232669027\n",
      "Average Specificity: 0.36912834136763567\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, recall_score, precision_score,\n",
    "    roc_auc_score, auc, precision_recall_curve, roc_curve, confusion_matrix\n",
    ")\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.base import clone\n",
    "from scipy.stats import bootstrap\n",
    "\n",
    "# Load and preprocess the dataset (ensure correct path and format)\n",
    "df = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/merged_mgh_bidmc222.csv')\n",
    "\n",
    "# Extract features (X) and target variable (y)\n",
    "X = df[['icd']]  # ICD codes column\n",
    "y = df['annot']  # Target variable\n",
    "\n",
    "#Handle class imbalance with RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Initialize K-Fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)  # 10 splits with shuffling\n",
    "model = LogisticRegression(class_weight='balanced', random_state=2025)  # Logistic regression model with balanced class weights\n",
    "\n",
    "# Lists to store results for each fold\n",
    "accuracies, f1_scores, recalls, precisions = [], [], [], []\n",
    "aurocs, auprcs, specificities = [], [], []\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in kf.split(X_resampled):\n",
    "    # Split the resampled data into training and test sets\n",
    "    X_train, X_test = X_resampled.iloc[train_index], X_resampled.iloc[test_index]\n",
    "    y_train, y_test = y_resampled.iloc[train_index], y_resampled.iloc[test_index]\n",
    "\n",
    "    # Clone the model to ensure independence for each fold\n",
    "    model_clone = clone(model)\n",
    "    model_clone.fit(X_train, y_train)  # Train the model\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model_clone.predict(X_test)\n",
    "    y_pred_proba = model_clone.predict_proba(X_test)[:, 1]  # Probability of the positive class\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "    recalls.append(recall_score(y_test, y_pred))\n",
    "    precisions.append(precision_score(y_test, y_pred))\n",
    "\n",
    "    # ROC and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    aurocs.append(auc(fpr, tpr))\n",
    "\n",
    "    # Precision-Recall curve and AUPRC\n",
    "    precision_points, recall_points, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    auprcs.append(auc(recall_points, precision_points))\n",
    "\n",
    "    # Specificity (True Negative Rate)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificities.append(tn / (tn + fp))\n",
    "\n",
    "# Function to compute 95% confidence intervals using bootstrap\n",
    "def calculate_ci(data):\n",
    "    result = bootstrap((np.array(data),), np.mean, confidence_level=0.95)\n",
    "    return result.confidence_interval\n",
    "\n",
    "# Print confidence intervals for each metric\n",
    "print(\"Accuracy CI:\", calculate_ci(accuracies))\n",
    "print(\"F1-score CI:\", calculate_ci(f1_scores))\n",
    "print(\"Recall CI:\", calculate_ci(recalls))\n",
    "print(\"Precision CI:\", calculate_ci(precisions))\n",
    "print(\"AUROC CI:\", calculate_ci(aurocs))\n",
    "print(\"AUPRC CI:\", calculate_ci(auprcs))\n",
    "print(\"Specificity CI:\", calculate_ci(specificities))\n",
    "\n",
    "# Print average values for each metric\n",
    "print(\"Average Accuracy:\", np.mean(accuracies))\n",
    "print(\"Average F1-score:\", np.mean(f1_scores))\n",
    "print(\"Average Recall:\", np.mean(recalls))\n",
    "print(\"Average Precision:\", np.mean(precisions))\n",
    "print(\"Average AUROC:\", np.mean(aurocs))\n",
    "print(\"Average AUPRC:\", np.mean(auprcs))\n",
    "print(\"Average Specificity:\", np.mean(specificities))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy CI: ConfidenceInterval(low=0.5251457695580649, high=0.5525849920732782)\n",
    "F1-score CI: ConfidenceInterval(low=0.5880069800519575, high=0.6204001727018401)\n",
    "Recall CI: ConfidenceInterval(low=0.6895476219930763, high=0.718962558867317)\n",
    "Precision CI: ConfidenceInterval(low=0.5112345655621887, high=0.5451753534821403)\n",
    "AUROC CI: ConfidenceInterval(low=0.5271609031629594, high=0.5496511963309062)\n",
    "AUPRC CI: ConfidenceInterval(low=0.6771440143635011, high=0.7036065982878332)\n",
    "Specificity CI: ConfidenceInterval(low=0.35460200634741884, high=0.38311053485680646)\n",
    "Average Accuracy: 0.536861686296186\n",
    "Average F1-score: 0.602855387552034\n",
    "Average Recall: 0.7039825385220351\n",
    "Average Precision: 0.5273178096220179\n",
    "Average AUROC: 0.5365554399448353\n",
    "Average AUPRC: 0.689491232669027\n",
    "Average Specificity: 0.36912834136763567"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UnoRub7-rsH"
   },
   "source": [
    "### 1.4 Med Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 754,
     "status": "ok",
     "timestamp": 1737650146881,
     "user": {
      "displayName": "Ruoqi Wei",
      "userId": "06248467831337262349"
     },
     "user_tz": 300
    },
    "id": "ClcBjBZ5BKH-",
    "outputId": "bf54e7ce-dd09-40dd-87ac-2eac433d68c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy CI: ConfidenceInterval(low=np.float64(0.5517078753209479), high=np.float64(0.5789836241005318))\n",
      "F1-score CI: ConfidenceInterval(low=np.float64(0.5976816340505097), high=np.float64(0.6297741967963978))\n",
      "Recall CI: ConfidenceInterval(low=np.float64(0.6756809622385578), high=np.float64(0.7120933404044725))\n",
      "Precision CI: ConfidenceInterval(low=np.float64(0.5336752476361454), high=np.float64(0.5688721250778443))\n",
      "ROC AUC CI: ConfidenceInterval(low=np.float64(0.553223091870324), high=np.float64(0.5769324804424608))\n",
      "AUPRC CI: ConfidenceInterval(low=np.float64(0.6849527009642313), high=np.float64(0.7120827643957545))\n",
      "Specificity CI: ConfidenceInterval(low=np.float64(0.4127859352732476), high=np.float64(0.44741206519189936))\n",
      "Average Accuracy: 0.562256503860101\n",
      "Average F1-score: 0.6129853415798636\n",
      "Average Recall: 0.6945198438368045\n",
      "Average Precision: 0.5490123483450897\n",
      "Average ROC AUC: 0.5621008090933352\n",
      "Average AUPRC: 0.6980109862405527\n",
      "Average Specificity: 0.4296817743498657\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, recall_score, precision_score,\n",
    "    roc_auc_score, auc, precision_recall_curve, roc_curve, confusion_matrix\n",
    ")\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.base import clone\n",
    "from scipy.stats import bootstrap\n",
    "df = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/merged_mgh_bidmc222.csv')\n",
    "\n",
    "\n",
    "# Assume the dataframe 'df' is preloaded and contains the necessary columns\n",
    "# Replace 'med' with the actual feature column and 'annot' with the target column\n",
    "features = df[['med']]  # Feature column\n",
    "target = df['annot']  # Target variable column\n",
    "\n",
    "# Handle class imbalance using RandomOverSampler\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "features_resampled, target_resampled = oversampler.fit_resample(features, target)\n",
    "\n",
    "# Initialize K-Fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)  # 10-fold cross-validation\n",
    "logistic_model = LogisticRegression(class_weight='balanced', random_state=2025)  # Logistic regression with balanced class weights\n",
    "\n",
    "# Lists to store evaluation metrics for each fold\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "roc_auc_scores = []\n",
    "auprc_scores = []\n",
    "specificity_scores = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_indices, test_indices in kf.split(features_resampled):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test = features_resampled.iloc[train_indices], features_resampled.iloc[test_indices]\n",
    "    y_train, y_test = target_resampled.iloc[train_indices], target_resampled.iloc[test_indices]\n",
    "\n",
    "    # Clone the model to ensure independence for each fold\n",
    "    model_clone = clone(logistic_model)\n",
    "    model_clone.fit(X_train, y_train)  # Train the model\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model_clone.predict(X_test)\n",
    "    y_pred_prob = model_clone.predict_proba(X_test)[:, 1]  # Probability of the positive class\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "    recall_scores.append(recall_score(y_test, y_pred))\n",
    "    precision_scores.append(precision_score(y_test, y_pred))\n",
    "\n",
    "    # ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    roc_auc_scores.append(auc(fpr, tpr))\n",
    "\n",
    "    # Precision-Recall curve and AUPRC\n",
    "    precision_points, recall_points, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "    auprc_scores.append(auc(recall_points, precision_points))\n",
    "\n",
    "    # Specificity (True Negative Rate)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    specificity_scores.append(specificity)\n",
    "\n",
    "# Function to calculate 95% confidence intervals using bootstrap\n",
    "def calculate_ci(metric_list):\n",
    "    result = bootstrap((np.array(metric_list),), np.mean, confidence_level=0.95)\n",
    "    return result.confidence_interval\n",
    "\n",
    "# Print confidence intervals for each metric\n",
    "print(\"Accuracy CI:\", calculate_ci(accuracy_scores))\n",
    "print(\"F1-score CI:\", calculate_ci(f1_scores))\n",
    "print(\"Recall CI:\", calculate_ci(recall_scores))\n",
    "print(\"Precision CI:\", calculate_ci(precision_scores))\n",
    "print(\"ROC AUC CI:\", calculate_ci(roc_auc_scores))\n",
    "print(\"AUPRC CI:\", calculate_ci(auprc_scores))\n",
    "print(\"Specificity CI:\", calculate_ci(specificity_scores))\n",
    "\n",
    "# Print average values for each metric\n",
    "print(\"Average Accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Average F1-score:\", np.mean(f1_scores))\n",
    "print(\"Average Recall:\", np.mean(recall_scores))\n",
    "print(\"Average Precision:\", np.mean(precision_scores))\n",
    "print(\"Average ROC AUC:\", np.mean(roc_auc_scores))\n",
    "print(\"Average AUPRC:\", np.mean(auprc_scores))\n",
    "print(\"Average Specificity:\", np.mean(specificity_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy CI: ConfidenceInterval(low=0.5522011079188255, high=0.5792735530615427)\n",
    "F1-score CI: ConfidenceInterval(low=0.5976484058577213, high=0.629865040069949)\n",
    "Recall CI: ConfidenceInterval(low=0.676377129174852, high=0.7114721275824842)\n",
    "Precision CI: ConfidenceInterval(low=0.5335043401507912, high=0.5690265235460552)\n",
    "ROC AUC CI: ConfidenceInterval(low=0.5531582611884539, high=0.5768187444821447)\n",
    "AUPRC CI: ConfidenceInterval(low=0.6854884054211079, high=0.7122810645493387)\n",
    "Specificity CI: ConfidenceInterval(low=0.4126565799296876, high=0.44753190808435445)\n",
    "Average Accuracy: 0.562256503860101\n",
    "Average F1-score: 0.6129853415798636\n",
    "Average Recall: 0.6945198438368045\n",
    "Average Precision: 0.5490123483450897\n",
    "Average ROC AUC: 0.5621008090933352\n",
    "Average AUPRC: 0.6980109862405527\n",
    "Average Specificity: 0.4296817743498657"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNWomVjtm3uvBT6rrITTr1q",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "10l69mzIvGtEou_30Waw7Qf0Tc_9N2-mT",
     "timestamp": 1737672171405
    }
   ]
  },
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
