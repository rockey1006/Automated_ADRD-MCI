{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCIXLUzLRIsV"
   },
   "source": [
    "## Table 3: Average performance and [95% confidence intervals] for logistic regression model using all features in the different testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVjT9O-nRbZE"
   },
   "source": [
    "## 1.1 trained on combined data [MGH + BIDMC], tested on combined data [MGH + BIDMC]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 414305,
     "status": "ok",
     "timestamp": 1737654567485,
     "user": {
      "displayName": "Ruoqi Wei",
      "userId": "06248467831337262349"
     },
     "user_tz": 300
    },
    "id": "Do9u_mLZEogI",
    "outputId": "a746d93f-2d5e-4441-cef8-56e8f5be6cc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier__C': 1.0, 'classifier__penalty': 'l1'}\n",
      "Best Score:  0.8941525083826667\n",
      "Accuracy: Mean=0.8885, 95% CI=(0.8756, 0.8982)\n",
      "Precision: Mean=0.8848, 95% CI=(0.8621, 0.9049)\n",
      "Recall: Mean=0.8754, 95% CI=(0.8594, 0.8909)\n",
      "F1: Mean=0.8800, 95% CI=(0.8615, 0.8877)\n",
      "Specificity: Mean=0.9000, 95% CI=(0.8843, 0.9204)\n",
      "Roc_auc: Mean=0.9476, 95% CI=(0.9374, 0.9552)\n",
      "Auprc: Mean=0.9472, 95% CI=(0.9343, 0.9542)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load training and test datasets\n",
    "train_data = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_bidmc_3948.csv')  # Training dataset\n",
    "test_data = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_bidmc_1664.csv')  # Test dataset\n",
    "\n",
    "# Define a text feature extractor using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))  # Unigrams, bigrams, and trigrams\n",
    "\n",
    "# Define class weights to handle class imbalance\n",
    "class_weight_dict = {0: 1.0, 1: 3.0}  # Higher weight for the positive class\n",
    "\n",
    "# Define the logistic regression classifier\n",
    "logistic_classifier = LogisticRegression(solver='liblinear', random_state=2025, class_weight=class_weight_dict)\n",
    "\n",
    "# Define a data preprocessor\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('text_vectorizer', tfidf_vectorizer, 'report_text'),  # Apply TF-IDF to the 'report_text' column\n",
    "    ('scaler', StandardScaler(), ['icd', 'med'])  # Scale numerical features 'icd' and 'med'\n",
    "], n_jobs=-1)\n",
    "\n",
    "# Create a pipeline for preprocessing and classification\n",
    "classification_pipeline = Pipeline([\n",
    "    ('preprocessor', data_preprocessor),\n",
    "    ('classifier', logistic_classifier)\n",
    "])\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "hyperparameter_grid = {\n",
    "    'classifier__penalty': ['l1'],  # L1 regularization\n",
    "    'classifier__C': [0.01, 0.1, 1.0, 10.0]  # Regularization strength\n",
    "}\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation to find the best hyperparameters\n",
    "grid_search1 = GridSearchCV(classification_pipeline, param_grid=hyperparameter_grid, cv=5, n_jobs=-1)\n",
    "grid_search1.fit(train_data[['icd', 'med', 'report_text']], train_data['annot'])\n",
    "\n",
    "# Display the best parameters and the best cross-validation score\n",
    "print(\"Best Parameters: \", grid_search1.best_params_)\n",
    "print(\"Best Score: \", grid_search1.best_score_)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "X_test = test_data[['icd', 'med', 'report_text']]  # Features\n",
    "y_test = test_data['annot']  # Target labels\n",
    "y_pred = grid_search1.predict(X_test)\n",
    "\n",
    "# Perform bootstrapping on the test set to calculate evaluation metrics\n",
    "n_iterations = 10  # Number of bootstrap iterations\n",
    "metrics_values = {\n",
    "    'accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': [],\n",
    "    'specificity': [],\n",
    "    'roc_auc': [],\n",
    "    'auprc': []\n",
    "}\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Sample the test data with replacement\n",
    "    sample_indices = np.random.choice(len(X_test), len(X_test), replace=True)\n",
    "    X_sampled = X_test.iloc[sample_indices]\n",
    "    y_sampled = y_test.iloc[sample_indices]\n",
    "\n",
    "    # Predict on the sampled data\n",
    "    y_pred_sampled = grid_search1.predict(X_sampled)\n",
    "    y_pred_prob_sampled = grid_search1.predict_proba(X_sampled)[:, 1]  # Probability of the positive class\n",
    "\n",
    "    # Calculate confusion matrix for specificity calculation\n",
    "    tn, fp, fn, tp = confusion_matrix(y_sampled, y_pred_sampled).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "\n",
    "    # Append calculated metrics to the respective lists\n",
    "    metrics_values['accuracy'].append(accuracy_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['precision'].append(precision_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['recall'].append(recall_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['f1'].append(f1_score(y_sampled, y_pred_sampled))\n",
    "    metrics_values['specificity'].append(specificity)\n",
    "    metrics_values['roc_auc'].append(roc_auc_score(y_sampled, y_pred_prob_sampled))\n",
    "    metrics_values['auprc'].append(average_precision_score(y_sampled, y_pred_prob_sampled))\n",
    "\n",
    "# Calculate mean and 95% confidence intervals for each metric\n",
    "for metric, values in metrics_values.items():\n",
    "    mean_value = np.mean(values)\n",
    "    lower_bound = np.percentile(values, 2.5)  # Lower bound of the 95% CI\n",
    "    upper_bound = np.percentile(values, 97.5)  # Upper bound of the 95% CI\n",
    "\n",
    "    print(f\"{metric.capitalize()}: Mean={mean_value:.4f}, 95% CI=({lower_bound:.4f}, {upper_bound:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters:  {'classifier__C': 1.0, 'classifier__penalty': 'l1'}\n",
    "Best Score:  0.8941525083826667\n",
    "Accuracy: Mean=0.8980, 95% CI=(0.8890, 0.9058)\n",
    "Precision: Mean=0.8939, 95% CI=(0.8750, 0.9061)\n",
    "Recall: Mean=0.8902, 95% CI=(0.8797, 0.9049)\n",
    "F1: Mean=0.8920, 95% CI=(0.8806, 0.9012)\n",
    "Specificity: Mean=0.9050, 95% CI=(0.8935, 0.9172)\n",
    "Roc_auc: Mean=0.9532, 95% CI=(0.9471, 0.9606)\n",
    "Auprc: Mean=0.9528, 95% CI=(0.9458, 0.9606)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoA7Cv63Saj6"
   },
   "source": [
    "## 1.2 trained on combined data [MGH + BIDMC], tested on MGH data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 338623,
     "status": "ok",
     "timestamp": 1737654907530,
     "user": {
      "displayName": "Ruoqi Wei",
      "userId": "06248467831337262349"
     },
     "user_tz": 300
    },
    "id": "MSohHp14GLNr",
    "outputId": "ec0ed5b7-e1e7-40a0-988d-bcd41331b30a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier__C': 1.0, 'classifier__penalty': 'l1'}\n",
      "Best Score:  0.8941525083826667\n",
      "Accuracy: Mean=0.9368, 95% CI=(0.9147, 0.9497)\n",
      "Precision: Mean=0.9624, 95% CI=(0.9489, 0.9788)\n",
      "Recall: Mean=0.9118, 95% CI=(0.8797, 0.9452)\n",
      "F1: Mean=0.9363, 95% CI=(0.9140, 0.9529)\n",
      "Specificity: Mean=0.9627, 95% CI=(0.9485, 0.9759)\n",
      "Roc_auc: Mean=0.9824, 95% CI=(0.9731, 0.9885)\n",
      "Auprc: Mean=0.9850, 95% CI=(0.9781, 0.9911)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_data = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_bidmc_3948.csv')  # Training dataset\n",
    "test_data = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_680.csv')  # Test dataset\n",
    "\n",
    "# Define the text feature extractor using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))  # Use unigrams, bigrams, and trigrams\n",
    "\n",
    "# Define class weights to address class imbalance\n",
    "class_weight_dict = {0: 1.0, 1: 3.0}  # Assign higher weight to the positive class\n",
    "\n",
    "# Define the logistic regression model\n",
    "logistic_model = LogisticRegression(solver='liblinear', random_state=2025, class_weight=class_weight_dict)\n",
    "\n",
    "# Define the data preprocessor\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('text_features', tfidf_vectorizer, 'report_text'),  # Apply TF-IDF to 'report_text'\n",
    "    ('scale_numeric', StandardScaler(), ['icd', 'med'])  # Scale numerical features 'icd' and 'med'\n",
    "], n_jobs=-1)\n",
    "\n",
    "# Create a pipeline for data preprocessing and classification\n",
    "classification_pipeline = Pipeline([\n",
    "    ('preprocessor', data_preprocessor),\n",
    "    ('classifier', logistic_model)\n",
    "])\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'classifier__penalty': ['l1'],  # Use L1 regularization\n",
    "    'classifier__C': [0.01, 0.1, 1.0, 10.0]  # Regularization strength\n",
    "}\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search2 = GridSearchCV(classification_pipeline, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search2.fit(train_data[['icd', 'med', 'report_text']], train_data['annot'])\n",
    "\n",
    "# Display the best parameters and corresponding score\n",
    "print(\"Best Parameters: \", grid_search2.best_params_)\n",
    "print(\"Best Score: \", grid_search2.best_score_)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "X_test = test_data[['icd', 'med', 'report_text']]  # Features\n",
    "y_test = test_data['annot']  # Target labels\n",
    "y_pred = grid_search2.predict(X_test)\n",
    "\n",
    "# Perform bootstrapping to compute evaluation metrics\n",
    "n_iterations = 10  # Number of bootstrap iterations\n",
    "metrics = {\n",
    "    'accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': [],\n",
    "    'specificity': [],\n",
    "    'roc_auc': [],\n",
    "    'auprc': []\n",
    "}\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Sample test data with replacement\n",
    "    sampled_indices = np.random.choice(len(X_test), len(X_test), replace=True)\n",
    "    X_sampled = X_test.iloc[sampled_indices]\n",
    "    y_sampled = y_test.iloc[sampled_indices]\n",
    "\n",
    "    # Predict on the sampled data\n",
    "    y_sampled_pred = grid_search2.predict(X_sampled)\n",
    "    y_sampled_prob = grid_search2.predict_proba(X_sampled)[:, 1]  # Probability of the positive class\n",
    "\n",
    "    # Calculate confusion matrix and specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_sampled, y_sampled_pred).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "\n",
    "    # Store metrics\n",
    "    metrics['accuracy'].append(accuracy_score(y_sampled, y_sampled_pred))\n",
    "    metrics['precision'].append(precision_score(y_sampled, y_sampled_pred))\n",
    "    metrics['recall'].append(recall_score(y_sampled, y_sampled_pred))\n",
    "    metrics['f1'].append(f1_score(y_sampled, y_sampled_pred))\n",
    "    metrics['specificity'].append(specificity)\n",
    "    metrics['roc_auc'].append(roc_auc_score(y_sampled, y_sampled_prob))\n",
    "    metrics['auprc'].append(average_precision_score(y_sampled, y_sampled_prob))\n",
    "\n",
    "# Calculate mean and 95% confidence intervals for each metric\n",
    "for metric_name, metric_values in metrics.items():\n",
    "    mean_value = np.mean(metric_values)\n",
    "    lower_bound = np.percentile(metric_values, 2.5)  # Lower bound of 95% CI\n",
    "    upper_bound = np.percentile(metric_values, 97.5)  # Upper bound of 95% CI\n",
    "\n",
    "    print(f\"{metric_name.capitalize()}: Mean={mean_value:.4f}, 95% CI=({lower_bound:.4f}, {upper_bound:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters:  {'classifier__C': 1.0, 'classifier__penalty': 'l1'}\n",
    "Best Score:  0.8941525083826667\n",
    "Accuracy: Mean=0.9321, 95% CI=(0.9191, 0.9461)\n",
    "Precision: Mean=0.9607, 95% CI=(0.9468, 0.9737)\n",
    "Recall: Mean=0.9053, 95% CI=(0.8741, 0.9339)\n",
    "F1: Mean=0.9320, 95% CI=(0.9160, 0.9455)\n",
    "Specificity: Mean=0.9606, 95% CI=(0.9471, 0.9742)\n",
    "Roc_auc: Mean=0.9803, 95% CI=(0.9705, 0.9864)\n",
    "Auprc: Mean=0.9843, 95% CI=(0.9771, 0.9892)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvaW-6nIUFl-"
   },
   "source": [
    "## 1.3 trained on combined data [MGH + BIDMC], tested on BIDMC data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 392814,
     "status": "ok",
     "timestamp": 1737655300335,
     "user": {
      "displayName": "Ruoqi Wei",
      "userId": "06248467831337262349"
     },
     "user_tz": 300
    },
    "id": "DRSwiYPkGlh6",
    "outputId": "2568f6c5-2296-4deb-d850-2f4c2888b784"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier__C': 1.0, 'classifier__penalty': 'l1'}\n",
      "Best Score:  0.8941525083826667\n",
      "Accuracy: Mean=0.8572, 95% CI=(0.8419, 0.8784)\n",
      "Precision: Mean=0.8206, 95% CI=(0.8045, 0.8586)\n",
      "Recall: Mean=0.8581, 95% CI=(0.8429, 0.8804)\n",
      "F1: Mean=0.8388, 95% CI=(0.8244, 0.8597)\n",
      "Specificity: Mean=0.8564, 95% CI=(0.8394, 0.8891)\n",
      "Roc_auc: Mean=0.9218, 95% CI=(0.9099, 0.9324)\n",
      "Auprc: Mean=0.9082, 95% CI=(0.8994, 0.9218)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_data = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_bidmc_3948.csv')  # Training dataset\n",
    "test_data = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/bidmc_984.csv')  # Test dataset\n",
    "\n",
    "# Define the text feature extractor using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))  # Use unigrams, bigrams, and trigrams\n",
    "\n",
    "# Define class weights to address class imbalance\n",
    "class_weight_dict = {0: 1.0, 1: 3.0}  # Assign higher weight to the positive class\n",
    "\n",
    "# Define the logistic regression classifier\n",
    "logistic_classifier = LogisticRegression(solver='liblinear', random_state=2025, class_weight=class_weight_dict)\n",
    "\n",
    "# Define the data preprocessor\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('text_features', tfidf_vectorizer, 'report_text'),  # Apply TF-IDF to 'report_text'\n",
    "    ('scale_numeric', StandardScaler(), ['icd', 'med'])  # Scale numerical features 'icd' and 'med'\n",
    "], n_jobs=-1)\n",
    "\n",
    "# Create a pipeline for preprocessing and classification\n",
    "classification_pipeline = Pipeline([\n",
    "    ('preprocessor', data_preprocessor),\n",
    "    ('classifier', logistic_classifier)\n",
    "])\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "hyperparameter_grid = {\n",
    "    'classifier__penalty': ['l1'],  # Use L1 regularization\n",
    "    'classifier__C': [0.01, 0.1, 1.0, 10.0]  # Regularization strength\n",
    "}\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search3 = GridSearchCV(classification_pipeline, param_grid=hyperparameter_grid, cv=5, n_jobs=-1)\n",
    "grid_search3.fit(train_data[['icd', 'med', 'report_text']], train_data['annot'])\n",
    "\n",
    "# Display the best hyperparameters and corresponding score\n",
    "print(\"Best Parameters: \", grid_search3.best_params_)\n",
    "print(\"Best Score: \", grid_search3.best_score_)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "X_test = test_data[['icd', 'med', 'report_text']]  # Features\n",
    "y_test = test_data['annot']  # Target labels\n",
    "y_pred = grid_search3.predict(X_test)\n",
    "\n",
    "# Perform bootstrapping to compute evaluation metrics\n",
    "n_iterations = 10  # Number of bootstrap iterations\n",
    "metrics = {\n",
    "    'accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': [],\n",
    "    'specificity': [],\n",
    "    'roc_auc': [],\n",
    "    'auprc': []\n",
    "}\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Sample test data with replacement\n",
    "    sampled_indices = np.random.choice(len(X_test), len(X_test), replace=True)\n",
    "    X_sampled = X_test.iloc[sampled_indices]\n",
    "    y_sampled = y_test.iloc[sampled_indices]\n",
    "\n",
    "    # Predict on the sampled data\n",
    "    y_sampled_pred = grid_search3.predict(X_sampled)\n",
    "    y_sampled_prob = grid_search3.predict_proba(X_sampled)[:, 1]  # Probability of the positive class\n",
    "\n",
    "    # Calculate confusion matrix and specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_sampled, y_sampled_pred).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "\n",
    "    # Store metrics\n",
    "    metrics['accuracy'].append(accuracy_score(y_sampled, y_sampled_pred))\n",
    "    metrics['precision'].append(precision_score(y_sampled, y_sampled_pred))\n",
    "    metrics['recall'].append(recall_score(y_sampled, y_sampled_pred))\n",
    "    metrics['f1'].append(f1_score(y_sampled, y_sampled_pred))\n",
    "    metrics['specificity'].append(specificity)\n",
    "    metrics['roc_auc'].append(roc_auc_score(y_sampled, y_sampled_prob))\n",
    "    metrics['auprc'].append(average_precision_score(y_sampled, y_sampled_prob))\n",
    "\n",
    "# Calculate mean and 95% confidence intervals for each metric\n",
    "for metric_name, metric_values in metrics.items():\n",
    "    mean_value = np.mean(metric_values)\n",
    "    lower_bound = np.percentile(metric_values, 2.5)  # Lower bound of 95% CI\n",
    "    upper_bound = np.percentile(metric_values, 97.5)  # Upper bound of 95% CI\n",
    "\n",
    "    print(f\"{metric_name.capitalize()}: Mean={mean_value:.4f}, 95% CI=({lower_bound:.4f}, {upper_bound:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters:  {'classifier__C': 1.0, 'classifier__penalty': 'l1'}\n",
    "Best Score:  0.8941525083826667\n",
    "Accuracy: Mean=0.8635, 95% CI=(0.8490, 0.8801)\n",
    "Precision: Mean=0.8248, 95% CI=(0.7973, 0.8512)\n",
    "Recall: Mean=0.8685, 95% CI=(0.8471, 0.8847)\n",
    "F1: Mean=0.8459, 95% CI=(0.8332, 0.8646)\n",
    "Specificity: Mean=0.8599, 95% CI=(0.8349, 0.8836)\n",
    "Roc_auc: Mean=0.9271, 95% CI=(0.9166, 0.9418)\n",
    "Auprc: Mean=0.9108, 95% CI=(0.8911, 0.9313)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPFpLttwUMOS"
   },
   "source": [
    "## 1.4 trained on BIDMC data, tested on MGH data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 451524,
     "status": "ok",
     "timestamp": 1737655751850,
     "user": {
      "displayName": "Ruoqi Wei",
      "userId": "06248467831337262349"
     },
     "user_tz": 300
    },
    "id": "qaojwJMnJT5Q",
    "outputId": "580b46f4-07ea-4344-9fdd-81e286e9fef7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier__C': 10.0, 'classifier__penalty': 'l1'}\n",
      "Best Score:  0.9137195121951219\n",
      "Accuracy: Mean=0.9431, 95% CI=(0.9350, 0.9523)\n",
      "Precision: Mean=0.9153, 95% CI=(0.9057, 0.9283)\n",
      "Recall: Mean=0.8937, 95% CI=(0.8772, 0.9115)\n",
      "F1: Mean=0.9043, 95% CI=(0.8913, 0.9180)\n",
      "Specificity: Mean=0.9643, 95% CI=(0.9600, 0.9717)\n",
      "Roc_auc: Mean=0.9767, 95% CI=(0.9696, 0.9848)\n",
      "Auprc: Mean=0.9584, 95% CI=(0.9428, 0.9711)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_data = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/bidmc_3280.csv')  # Training dataset\n",
    "test_data = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_2332.csv')  # Test dataset\n",
    "\n",
    "# Define the text feature extractor using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))  # Use unigrams, bigrams, and trigrams\n",
    "\n",
    "# Define class weights to address class imbalance\n",
    "class_weight_dict = {0: 1.0, 1: 3.0}  # Assign higher weight to the positive class\n",
    "\n",
    "# Define the logistic regression classifier\n",
    "logistic_classifier = LogisticRegression(solver='liblinear', random_state=2025, class_weight=class_weight_dict)\n",
    "\n",
    "# Define the data preprocessor\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('text_features', tfidf_vectorizer, 'report_text'),  # Apply TF-IDF to 'report_text'\n",
    "    ('scale_numeric', StandardScaler(), ['icd', 'med'])  # Scale numerical features 'icd' and 'med'\n",
    "], n_jobs=-1)\n",
    "\n",
    "# Create a pipeline for preprocessing and classification\n",
    "classification_pipeline = Pipeline([\n",
    "    ('preprocessor', data_preprocessor),\n",
    "    ('classifier', logistic_classifier)\n",
    "])\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "hyperparameter_grid = {\n",
    "    'classifier__penalty': ['l1'],  # Use L1 regularization\n",
    "    'classifier__C': [0.01, 0.1, 1.0, 10.0]  # Regularization strength\n",
    "}\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search4 = GridSearchCV(classification_pipeline, param_grid=hyperparameter_grid, cv=5, n_jobs=-1)\n",
    "grid_search4.fit(train_data[['icd', 'med', 'report_text']], train_data['annot'])\n",
    "\n",
    "# Display the best hyperparameters and corresponding score\n",
    "print(\"Best Parameters: \", grid_search4.best_params_)\n",
    "print(\"Best Score: \", grid_search4.best_score_)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "X_test = test_data[['icd', 'med', 'report_text']]  # Features from test data\n",
    "y_test = test_data['annot']  # Target labels from test data\n",
    "y_pred = grid_search4.predict(X_test)\n",
    "\n",
    "# Perform bootstrapping to compute evaluation metrics\n",
    "n_iterations = 10  # Number of bootstrap iterations\n",
    "metrics = {\n",
    "    'accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': [],\n",
    "    'specificity': [],\n",
    "    'roc_auc': [],\n",
    "    'auprc': []\n",
    "}\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Sample test data with replacement\n",
    "    sampled_indices = np.random.choice(len(X_test), len(X_test), replace=True)\n",
    "    X_sampled = X_test.iloc[sampled_indices]\n",
    "    y_sampled = y_test.iloc[sampled_indices]\n",
    "\n",
    "    # Predict on the sampled data\n",
    "    y_sampled_pred = grid_search4.predict(X_sampled)\n",
    "    y_sampled_prob = grid_search4.predict_proba(X_sampled)[:, 1]  # Probability of the positive class\n",
    "\n",
    "    # Calculate confusion matrix and specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_sampled, y_sampled_pred).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "\n",
    "    # Store metrics\n",
    "    metrics['accuracy'].append(accuracy_score(y_sampled, y_sampled_pred))\n",
    "    metrics['precision'].append(precision_score(y_sampled, y_sampled_pred))\n",
    "    metrics['recall'].append(recall_score(y_sampled, y_sampled_pred))\n",
    "    metrics['f1'].append(f1_score(y_sampled, y_sampled_pred))\n",
    "    metrics['specificity'].append(specificity)\n",
    "    metrics['roc_auc'].append(roc_auc_score(y_sampled, y_sampled_prob))\n",
    "    metrics['auprc'].append(average_precision_score(y_sampled, y_sampled_prob))\n",
    "\n",
    "# Calculate mean and 95% confidence intervals for each metric\n",
    "for metric_name, metric_values in metrics.items():\n",
    "    mean_value = np.mean(metric_values)\n",
    "    lower_bound = np.percentile(metric_values, 2.5)  # Lower bound of 95% CI\n",
    "    upper_bound = np.percentile(metric_values, 97.5)  # Upper bound of 95% CI\n",
    "\n",
    "    print(f\"{metric_name.capitalize()}: Mean={mean_value:.4f}, 95% CI=({lower_bound:.4f}, {upper_bound:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters:  {'classifier__C': 10.0, 'classifier__penalty': 'l1'}\n",
    "Best Score:  0.9137195121951219\n",
    "Accuracy: Mean=0.9448, 95% CI=(0.9397, 0.9500)\n",
    "Precision: Mean=0.9092, 95% CI=(0.8964, 0.9171)\n",
    "Recall: Mean=0.9032, 95% CI=(0.8882, 0.9196)\n",
    "F1: Mean=0.9062, 95% CI=(0.8960, 0.9140)\n",
    "Specificity: Mean=0.9622, 95% CI=(0.9574, 0.9670)\n",
    "Roc_auc: Mean=0.9774, 95% CI=(0.9735, 0.9806)\n",
    "Auprc: Mean=0.9581, 95% CI=(0.9495, 0.9628)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8dR-RNLUSQN"
   },
   "source": [
    "## 1.5 trained on MGH data, tested on BIDMC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 273999,
     "status": "ok",
     "timestamp": 1737656025839,
     "user": {
      "displayName": "Ruoqi Wei",
      "userId": "06248467831337262349"
     },
     "user_tz": 300
    },
    "id": "S_wsxJoiJk4m",
    "outputId": "9eb61afb-4259-47a6-c1b8-c5e687a40345"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier__C': 10.0, 'classifier__penalty': 'l1'}\n",
      "Best Score:  0.9502495152144543\n",
      "Accuracy: Mean=0.9104, 95% CI=(0.9035, 0.9180)\n",
      "Precision: Mean=0.7975, 95% CI=(0.7793, 0.8144)\n",
      "Recall: Mean=0.8788, 95% CI=(0.8671, 0.8910)\n",
      "F1: Mean=0.8361, 95% CI=(0.8221, 0.8499)\n",
      "Specificity: Mean=0.9215, 95% CI=(0.9151, 0.9282)\n",
      "Roc_auc: Mean=0.9457, 95% CI=(0.9384, 0.9519)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_data = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_2332.csv')  # Training dataset\n",
    "test_data = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/bidmc_3280.csv')  # Test dataset\n",
    "\n",
    "# Define the text feature extractor using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))  # Extract unigrams, bigrams, and trigrams\n",
    "\n",
    "# Define class weights to address class imbalance\n",
    "class_weight_dict = {0: 1.0, 1: 3.0}  # Assign higher weight to the positive class\n",
    "\n",
    "# Define the logistic regression classifier\n",
    "logistic_classifier = LogisticRegression(solver='liblinear', random_state=2025, class_weight=class_weight_dict)\n",
    "\n",
    "# Define the data preprocessor\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('text_features', tfidf_vectorizer, 'report_text'),  # Apply TF-IDF to 'report_text'\n",
    "    ('scale_numeric', StandardScaler(), ['icd', 'med'])  # Scale numerical features 'icd' and 'med'\n",
    "], n_jobs=-1)\n",
    "\n",
    "# Create a pipeline for preprocessing and classification\n",
    "classification_pipeline = Pipeline([\n",
    "    ('preprocessor', data_preprocessor),\n",
    "    ('classifier', logistic_classifier)\n",
    "])\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "hyperparameter_grid = {\n",
    "    'classifier__penalty': ['l1'],  # Use L1 regularization\n",
    "    'classifier__C': [0.01, 0.1, 1.0, 10.0]  # Regularization strength\n",
    "}\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search5 = GridSearchCV(classification_pipeline, param_grid=hyperparameter_grid, cv=5, n_jobs=-1)\n",
    "grid_search5.fit(train_data[['icd', 'med', 'report_text']], train_data['annot'])\n",
    "\n",
    "# Display the best parameters and corresponding score\n",
    "print(\"Best Parameters: \", grid_search5.best_params_)\n",
    "print(\"Best Score: \", grid_search5.best_score_)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "X_test = test_data[['icd', 'med', 'report_text']]  # Features\n",
    "y_test = test_data['annot']  # Target labels\n",
    "y_pred = grid_search5.predict(X_test)\n",
    "\n",
    "# Perform bootstrapping to compute evaluation metrics\n",
    "n_iterations = 10  # Number of bootstrap iterations\n",
    "metrics = {\n",
    "    'accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': [],\n",
    "    'specificity': [],\n",
    "    'roc_auc': []\n",
    "}\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Sample test data with replacement\n",
    "    sampled_indices = np.random.choice(len(X_test), len(X_test), replace=True)\n",
    "    X_sampled = X_test.iloc[sampled_indices]\n",
    "    y_sampled = y_test.iloc[sampled_indices]\n",
    "\n",
    "    # Predict on the sampled data\n",
    "    y_sampled_pred = grid_search5.predict(X_sampled)\n",
    "    y_sampled_prob = grid_search5.predict_proba(X_sampled)[:, 1]  # Probability of the positive class\n",
    "\n",
    "    # Calculate confusion matrix and specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_sampled, y_sampled_pred).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "\n",
    "    # Store metrics\n",
    "    metrics['accuracy'].append(accuracy_score(y_sampled, y_sampled_pred))\n",
    "    metrics['precision'].append(precision_score(y_sampled, y_sampled_pred))\n",
    "    metrics['recall'].append(recall_score(y_sampled, y_sampled_pred))\n",
    "    metrics['f1'].append(f1_score(y_sampled, y_sampled_pred))\n",
    "    metrics['specificity'].append(specificity)\n",
    "    metrics['roc_auc'].append(roc_auc_score(y_sampled, y_sampled_prob))\n",
    "\n",
    "# Calculate mean and confidence interval for each metric\n",
    "for metric_name, metric_values in metrics.items():\n",
    "    mean_value = np.mean(metric_values)\n",
    "    lower_bound = np.percentile(metric_values, 2.5)  # Lower bound of 95% CI\n",
    "    upper_bound = np.percentile(metric_values, 97.5)  # Upper bound of 95% CI\n",
    "\n",
    "    print(f\"{metric_name.capitalize()}: Mean={mean_value:.4f}, 95% CI=({lower_bound:.4f}, {upper_bound:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters:  {'classifier__C': 10.0, 'classifier__penalty': 'l1'}\n",
    "Best Score:  0.9502495152144543\n",
    "Accuracy: Mean=0.9108, 95% CI=(0.9043, 0.9231)\n",
    "Precision: Mean=0.8019, 95% CI=(0.7760, 0.8262)\n",
    "Recall: Mean=0.8757, 95% CI=(0.8528, 0.8965)\n",
    "F1: Mean=0.8371, 95% CI=(0.8239, 0.8593)\n",
    "Specificity: Mean=0.9233, 95% CI=(0.9161, 0.9334)\n",
    "Roc_auc: Mean=0.9449, 95% CI=(0.9347, 0.9550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 101807,
     "status": "ok",
     "timestamp": 1737661405460,
     "user": {
      "displayName": "Ruoqi Wei",
      "userId": "06248467831337262349"
     },
     "user_tz": 300
    },
    "id": "rDE4V-FXqvpZ",
    "outputId": "9605dcae-aa3d-4df3-a966-2bca6636d0fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier__C': 10.0, 'classifier__penalty': 'l1'}\n",
      "Best Score:  0.9582312551496841\n",
      "Accuracy: Mean=0.9465, 95% CI=(0.9397, 0.9541)\n",
      "Precision: Mean=0.9812, 95% CI=(0.9708, 0.9908)\n",
      "Recall: Mean=0.9148, 95% CI=(0.9010, 0.9276)\n",
      "F1: Mean=0.9468, 95% CI=(0.9410, 0.9541)\n",
      "Specificity: Mean=0.9810, 95% CI=(0.9699, 0.9908)\n",
      "Roc_auc: Mean=0.9812, 95% CI=(0.9758, 0.9861)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_data = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_1652.csv')  # Training dataset\n",
    "test_data = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/mgh_680.csv')  # Test dataset\n",
    "\n",
    "# Define the text feature extractor using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))  # Extract unigrams, bigrams, and trigrams\n",
    "\n",
    "# Define class weights to address class imbalance\n",
    "class_weight_dict = {0: 1.0, 1: 3.0}  # Assign higher weight to the positive class\n",
    "\n",
    "# Define the logistic regression classifier\n",
    "logistic_classifier = LogisticRegression(solver='liblinear', random_state=2025, class_weight=class_weight_dict)\n",
    "\n",
    "# Define the data preprocessor\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('text_features', tfidf_vectorizer, 'report_text'),  # Apply TF-IDF to 'report_text'\n",
    "    ('scale_numeric', StandardScaler(), ['icd', 'med'])  # Scale numerical features 'icd' and 'med'\n",
    "], n_jobs=-1)\n",
    "\n",
    "# Create a pipeline for preprocessing and classification\n",
    "classification_pipeline = Pipeline([\n",
    "    ('preprocessor', data_preprocessor),\n",
    "    ('classifier', logistic_classifier)\n",
    "])\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "hyperparameter_grid = {\n",
    "    'classifier__penalty': ['l1'],  # Use L1 regularization\n",
    "    'classifier__C': [0.01, 0.1, 1.0, 10.0]  # Regularization strength\n",
    "}\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search6 = GridSearchCV(classification_pipeline, param_grid=hyperparameter_grid, cv=5, n_jobs=-1)\n",
    "grid_search6.fit(train_data[['icd', 'med', 'report_text']], train_data['annot'])\n",
    "\n",
    "# Display the best parameters and corresponding score\n",
    "print(\"Best Parameters: \", grid_search6.best_params_)\n",
    "print(\"Best Score: \", grid_search6.best_score_)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "X_test = test_data[['icd', 'med', 'report_text']]  # Features\n",
    "y_test = test_data['annot']  # Target labels\n",
    "y_pred = grid_search6.predict(X_test)\n",
    "\n",
    "# Perform bootstrapping to compute evaluation metrics\n",
    "n_iterations = 10  # Number of bootstrap iterations\n",
    "metrics = {\n",
    "    'accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': [],\n",
    "    'specificity': [],\n",
    "    'roc_auc': []\n",
    "}\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Sample test data with replacement\n",
    "    sampled_indices = np.random.choice(len(X_test), len(X_test), replace=True)\n",
    "    X_sampled = X_test.iloc[sampled_indices]\n",
    "    y_sampled = y_test.iloc[sampled_indices]\n",
    "\n",
    "    # Predict on the sampled data\n",
    "    y_sampled_pred = grid_search6.predict(X_sampled)\n",
    "    y_sampled_prob = grid_search6.predict_proba(X_sampled)[:, 1]  # Probability of the positive class\n",
    "\n",
    "    # Calculate confusion matrix and specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_sampled, y_sampled_pred).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "\n",
    "    # Store metrics\n",
    "    metrics['accuracy'].append(accuracy_score(y_sampled, y_sampled_pred))\n",
    "    metrics['precision'].append(precision_score(y_sampled, y_sampled_pred))\n",
    "    metrics['recall'].append(recall_score(y_sampled, y_sampled_pred))\n",
    "    metrics['f1'].append(f1_score(y_sampled, y_sampled_pred))\n",
    "    metrics['specificity'].append(specificity)\n",
    "    metrics['roc_auc'].append(roc_auc_score(y_sampled, y_sampled_prob))\n",
    "\n",
    "# Calculate mean and confidence interval for each metric\n",
    "for metric_name, metric_values in metrics.items():\n",
    "    mean_value = np.mean(metric_values)\n",
    "    lower_bound = np.percentile(metric_values, 2.5)  # Lower bound of 95% CI\n",
    "    upper_bound = np.percentile(metric_values, 97.5)  # Upper bound of 95% CI\n",
    "\n",
    "    print(f\"{metric_name.capitalize()}: Mean={mean_value:.4f}, 95% CI=({lower_bound:.4f}, {upper_bound:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters:  {'classifier__C': 10.0, 'classifier__penalty': 'l1'}\n",
    "Best Score:  0.9582312551496841\n",
    "Accuracy: Mean=0.9513, 95% CI=(0.9471, 0.9570)\n",
    "Precision: Mean=0.9848, 95% CI=(0.9727, 0.9957)\n",
    "Recall: Mean=0.9187, 95% CI=(0.9020, 0.9282)\n",
    "F1: Mean=0.9505, 95% CI=(0.9433, 0.9567)\n",
    "Specificity: Mean=0.9854, 95% CI=(0.9737, 0.9956)\n",
    "Roc_auc: Mean=0.9831, 95% CI=(0.9785, 0.9890)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35551,
     "status": "ok",
     "timestamp": 1737661710726,
     "user": {
      "displayName": "Ruoqi Wei",
      "userId": "06248467831337262349"
     },
     "user_tz": 300
    },
    "id": "V2TbfGnhrFDZ",
    "outputId": "91d4e812-9d34-4446-c84f-0419275e7206"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier__C': 10.0, 'classifier__penalty': 'l1'}\n",
      "Best Score:  0.9011149000663068\n",
      "Accuracy: Mean=0.8796, 95% CI=(0.8597, 0.8997)\n",
      "Precision: Mean=0.8337, 95% CI=(0.8104, 0.8622)\n",
      "Recall: Mean=0.9062, 95% CI=(0.8771, 0.9377)\n",
      "F1: Mean=0.8683, 95% CI=(0.8442, 0.8917)\n",
      "Specificity: Mean=0.8591, 95% CI=(0.8428, 0.8809)\n",
      "Roc_auc: Mean=0.9472, 95% CI=(0.9324, 0.9600)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_data = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/bidmc_2296.csv')  # Training dataset\n",
    "test_data = pd.read_csv('/home/niels/cdac Dropbox/Niels Turley/codes/data/bidmc_984.csv')  # Test dataset\n",
    "\n",
    "# Define the text feature extractor using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))  # Extract unigrams, bigrams, and trigrams\n",
    "\n",
    "# Define class weights to address class imbalance\n",
    "class_weight_dict = {0: 1.0, 1: 3.0}  # Assign higher weight to the positive class\n",
    "\n",
    "# Define the logistic regression classifier\n",
    "logistic_classifier = LogisticRegression(solver='liblinear', random_state=2025, class_weight=class_weight_dict)\n",
    "\n",
    "# Define the data preprocessor\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('text_features', tfidf_vectorizer, 'report_text'),  # Apply TF-IDF to 'report_text'\n",
    "    ('scale_numeric', StandardScaler(), ['icd', 'med'])  # Scale numerical features 'icd' and 'med'\n",
    "], n_jobs=-1)\n",
    "\n",
    "# Create a pipeline for preprocessing and classification\n",
    "classification_pipeline = Pipeline([\n",
    "    ('preprocessor', data_preprocessor),\n",
    "    ('classifier', logistic_classifier)\n",
    "])\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "hyperparameter_grid = {\n",
    "    'classifier__penalty': ['l1'],  # Use L1 regularization\n",
    "    'classifier__C': [0.01, 0.1, 1.0, 10.0]  # Regularization strength\n",
    "}\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search7 = GridSearchCV(classification_pipeline, param_grid=hyperparameter_grid, cv=5, n_jobs=-1)\n",
    "grid_search7.fit(train_data[['icd', 'med', 'report_text']], train_data['annot'])\n",
    "\n",
    "# Display the best parameters and corresponding score\n",
    "print(\"Best Parameters: \", grid_search7.best_params_)\n",
    "print(\"Best Score: \", grid_search7.best_score_)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "X_test = test_data[['icd', 'med', 'report_text']]  # Features\n",
    "y_test = test_data['annot']  # Target labels\n",
    "y_pred = grid_search7.predict(X_test)\n",
    "\n",
    "# Perform bootstrapping to compute evaluation metrics\n",
    "n_iterations = 10  # Number of bootstrap iterations\n",
    "metrics = {\n",
    "    'accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': [],\n",
    "    'specificity': [],\n",
    "    'roc_auc': []\n",
    "}\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    # Sample test data with replacement\n",
    "    sampled_indices = np.random.choice(len(X_test), len(X_test), replace=True)\n",
    "    X_sampled = X_test.iloc[sampled_indices]\n",
    "    y_sampled = y_test.iloc[sampled_indices]\n",
    "\n",
    "    # Predict on the sampled data\n",
    "    y_sampled_pred = grid_search7.predict(X_sampled)\n",
    "    y_sampled_prob = grid_search7.predict_proba(X_sampled)[:, 1]  # Probability of the positive class\n",
    "\n",
    "    # Calculate confusion matrix and specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_sampled, y_sampled_pred).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "\n",
    "    # Store metrics\n",
    "    metrics['accuracy'].append(accuracy_score(y_sampled, y_sampled_pred))\n",
    "    metrics['precision'].append(precision_score(y_sampled, y_sampled_pred))\n",
    "    metrics['recall'].append(recall_score(y_sampled, y_sampled_pred))\n",
    "    metrics['f1'].append(f1_score(y_sampled, y_sampled_pred))\n",
    "    metrics['specificity'].append(specificity)\n",
    "    metrics['roc_auc'].append(roc_auc_score(y_sampled, y_sampled_prob))\n",
    "\n",
    "# Calculate mean and confidence interval for each metric\n",
    "for metric_name, metric_values in metrics.items():\n",
    "    mean_value = np.mean(metric_values)\n",
    "    lower_bound = np.percentile(metric_values, 2.5)  # Lower bound of 95% CI\n",
    "    upper_bound = np.percentile(metric_values, 97.5)  # Upper bound of 95% CI\n",
    "\n",
    "    print(f\"{metric_name.capitalize()}: Mean={mean_value:.4f}, 95% CI=({lower_bound:.4f}, {upper_bound:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Parameters:  {'classifier__C': 10.0, 'classifier__penalty': 'l1'}\n",
    "Best Score:  0.9011149000663068\n",
    "Accuracy: Mean=0.8815, 95% CI=(0.8672, 0.8963)\n",
    "Precision: Mean=0.8308, 95% CI=(0.8048, 0.8647)\n",
    "Recall: Mean=0.9144, 95% CI=(0.9087, 0.9260)\n",
    "F1: Mean=0.8705, 95% CI=(0.8557, 0.8873)\n",
    "Specificity: Mean=0.8563, 95% CI=(0.8322, 0.8842)\n",
    "Roc_auc: Mean=0.9510, 95% CI=(0.9398, 0.9592)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qlMIYlzX0Ny"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN8Z/F8E1Z8sVVwOZCmesoz",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1Tje7XpBZ-3N1XT-vF4YboFAdOUAK1e3d",
     "timestamp": 1737672587264
    }
   ]
  },
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
